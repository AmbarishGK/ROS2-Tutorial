{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ROS 2 Connected Course (Humble, Jammy)","text":"<p>This ROS 2 course is designed to run on Linux, Windows (WSL2 + WSLg), and macOS (XQuartz) - all using one Docker image.</p> <p>This repository contains tutorials, example code, and a Docker environment to help you learn and experiment with ROS 2 Humble Hawksbill and related tools (RViz2, Gazebo Classic, colcon, rosdep, etc.).</p> <p>For more information about ROS 2 Humble, visit the official documentation: \ud83d\udc49 https://docs.ros.org/en/humble</p>"},{"location":"#course-overview","title":"\ud83d\udcd8 Course Overview","text":"<p>This project is part of coursework for CMPE 249 - Intelligent Autonomous Systems at San Jos\u00e9 State University (SJSU).</p> <p>It provides a structured, classroom-ready introduction to ROS 2 with practical exercises in simulation, perception, and control.</p> <ul> <li>Start with Tutorial 0 \u2192 Pull and run the Docker container.  </li> <li>Then follow Tutorials 1-13 in order for progressive learning.  </li> <li>Compatible with Linux, Windows, and macOS using containerization.</li> </ul>"},{"location":"#docker-image-information","title":"\ud83d\udc33 Docker Image Information","text":"<p>Pre-built multi-architecture image available on Docker Hub:</p> <p>Repository: <pre><code>ambarishgk007/ros2-humble-rviz-gazebo:jammy\n</code></pre></p> <p>Pull command: <pre><code>docker pull ambarishgk007/ros2-humble-rviz-gazebo:jammy\n</code></pre></p> <p>The image automatically detects and runs on: - <code>amd64</code> (Intel / AMD laptops &amp; desktops) - <code>arm64</code> (Apple Silicon, Jetson boards, etc.)</p> <p>No authentication or keys are needed - the image is public.</p>"},{"location":"#installing-docker","title":"\u2699\ufe0f Installing Docker","text":""},{"location":"#linux-ubuntu-debian","title":"Linux (Ubuntu / Debian)","text":"<p><pre><code>sudo apt update\nsudo apt install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> Then add yourself to the Docker group: <pre><code>sudo usermod -aG docker $USER\nnewgrp docker\n</code></pre></p>"},{"location":"#windows-11-10","title":"Windows 11 / 10","text":"<ol> <li>Install Docker Desktop \u2192 https://www.docker.com/products/docker-desktop </li> <li>Enable WSL2 and install Ubuntu from the Microsoft Store.  </li> <li>Open Ubuntu (WSL2) and follow the Linux quick start below.</li> </ol>"},{"location":"#macos","title":"macOS","text":"<ol> <li>Install Docker Desktop for Mac \u2192 https://www.docker.com/products/docker-desktop </li> <li>Install XQuartz for GUI apps \u2192 https://www.xquartz.org/ </li> <li>Restart your Mac after installation.</li> </ol>"},{"location":"#quick-start-guide","title":"\ud83d\ude80 Quick Start Guide","text":""},{"location":"#linux-x11","title":"\ud83d\udc27 Linux (X11)","text":"<pre><code>xhost +local:\ndocker run -it --rm --name ros2course   -e DISPLAY=$DISPLAY -e QT_X11_NO_MITSHM=1   -v /tmp/.X11-unix:/tmp/.X11-unix:ro   -v $(pwd):/home/ros/ros2_tutorial   --device /dev/dri   ambarishgk007/ros2-humble-rviz-gazebo:jammy\n</code></pre>"},{"location":"#windows-11-wsl2-wslg","title":"\ud83e\ude9f Windows 11 (WSL2 + WSLg)","text":"<pre><code>docker run -it --rm --name ros2course   -e DISPLAY=$DISPLAY   -e WAYLAND_DISPLAY=$WAYLAND_DISPLAY   -e XDG_RUNTIME_DIR=$XDG_RUNTIME_DIR   -e PULSE_SERVER=$PULSE_SERVER   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /mnt/wslg:/mnt/wslg   -v $(pwd):/home/ros/ros2_tutorial   --device /dev/dri --device /dev/dxg   ambarishgk007/ros2-humble-rviz-gazebo:jammy\n</code></pre>"},{"location":"#macos-xquartz","title":"\ud83c\udf4e macOS (XQuartz)","text":"<pre><code>xhost + 127.0.0.1\ndocker run -it --rm --name ros2course   -e DISPLAY=host.docker.internal:0   -e QT_X11_NO_MITSHM=1   -v $(pwd):/home/ros/ros2_tutorial   ambarishgk007/ros2-humble-rviz-gazebo:jammy\n</code></pre>"},{"location":"#using-the-container","title":"\ud83e\udde0 Using the Container","text":"<p>Open a new terminal into the same container: <pre><code>docker exec -it ros2course bash\nsource /opt/ros/humble/setup.bash\n</code></pre></p> <p>Inside the container you can run: <pre><code>rviz2                # Open RViz2 GUI\ngazebo               # Open Gazebo Classic\nros2 run demo_nodes_cpp talker\nros2 run demo_nodes_cpp listener\n</code></pre></p>"},{"location":"#verify-gui","title":"\ud83d\udd0d Verify GUI","text":"<pre><code>xeyes        # tiny X11 test window\nglxinfo -B   # check OpenGL renderer info\n</code></pre>"},{"location":"#tutorials","title":"\ud83d\udcda Tutorials","text":"<p>All tutorials are in the <code>docs/</code> folder and published online:</p> <p>\ud83d\udc49 https://ambarishgk007.github.io/ROS2-Tutorial/</p> Tutorial Topic 0 Getting Started 1 Nodes, Topics, and Services 2 Parameters and Launch Files 3 Create a Package (Pub/Sub) 4 Services and Actions 5 TF2 and RViz2 6 URDF and Robot Description 7 Gazebo - Empty World &amp; Spawn 8 Gazebo Sensors and Plugins 9 Navigation (Nav2) 10 Record &amp; Replay (rosbag2) 11 RQT, RViz, and TF Debugging 12 Custom Interfaces and Lifecycle 13 Multi-Robot and Networking"},{"location":"#support-contribution","title":"\ud83e\udde9 Support &amp; Contribution","text":"<ul> <li>To fix typos or contribute new tutorials, open a Pull Request.  </li> <li>To rebuild the Docker image locally:   <pre><code>docker build -t ambarishgk007/ros2-humble-rviz-gazebo:jammy -f docker/Dockerfile .\n</code></pre></li> </ul>"},{"location":"#summary","title":"\ud83c\udfc1 Summary","text":"<p>\u2714\ufe0f Prebuilt ROS 2 Humble environment with GUI tools \u2714\ufe0f Works across Linux, macOS, and Windows \u2714\ufe0f Includes all core tutorials 0-13 \u2714\ufe0f Open-source and free to use for learning  </p> <p>Enjoy your journey through ROS 2 Connected Course!</p>"},{"location":"#course-information-acknowledgment","title":"\ud83c\udf93 Course Information &amp; Acknowledgment","text":"<p>This project was developed as part of the course CMPE 249 - Intelligent Autonomous Systems Department of Computer Engineering, San Jos\u00e9 State University (SJSU)</p> <p>Instructor: Dr. Kaikai Liu - Course Instructor, CMPE 249 (Guided the course framework, robotics lab setup, and simulation-based learning environment.)</p> <p>Student / Developer: Ambarish G.K - Developer of the ROS 2 Connected Course environment, Docker setup, and documentation.  </p> <p>Term: Fall 2025 Institution: San Jos\u00e9 State University, California, USA  </p> <p>Author: Ambarish G.K LinkedIn: Ambarish G.K Docker Hub: ambarishgk007/ros2-humble-rviz-gazebo</p>"},{"location":"0_getting_started/","title":"Tutorial 0: Getting Started - Pull, Run, and Use the Course Container","text":"<p>This course uses a prebuilt Docker image so everyone has the same working environment on Linux, Windows (WSL2 + WSLg), and macOS (XQuartz).</p> <ul> <li>Image: <code>ambarishgk007/ros2-humble-rviz-gazebo:jammy</code> (multi\u2011arch: amd64 + arm64)</li> <li>Container name (we\u2019ll reuse it): <code>ros2course</code></li> </ul> <p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"0_getting_started/#pull-the-image-all-os","title":"Pull the image (all OS)","text":"<pre><code>docker pull ambarishgk007/ros2-humble-rviz-gazebo:jammy\n</code></pre>"},{"location":"0_getting_started/#run-the-container-choose-your-os","title":"Run the container (choose your OS)","text":""},{"location":"0_getting_started/#linux-x11","title":"Linux (X11)","text":"<pre><code>xhost +local:\ndocker run -it --rm   --name ros2course   -e DISPLAY=$DISPLAY -e QT_X11_NO_MITSHM=1   -v /tmp/.X11-unix:/tmp/.X11-unix:ro   -v $(pwd):/home/ros/ros2_tutorial   --device /dev/dri   ambarishgk007/ros2-humble-rviz-gazebo:jammy\n</code></pre>"},{"location":"0_getting_started/#windows-11-wsl2-wslg-run-inside-your-wsl-ubuntu","title":"Windows 11 (WSL2 + WSLg) - run inside your WSL Ubuntu","text":"<pre><code>docker run -it --rm   --name ros2course   -e DISPLAY=$DISPLAY   -e WAYLAND_DISPLAY=$WAYLAND_DISPLAY   -e XDG_RUNTIME_DIR=$XDG_RUNTIME_DIR   -e PULSE_SERVER=$PULSE_SERVER   -v /tmp/.X11-unix:/tmp/.X11-unix   -v /mnt/wslg:/mnt/wslg   -v $(pwd):/home/ros/ros2_tutorial   --device /dev/dri --device /dev/dxg   ambarishgk007/ros2-humble-rviz-gazebo:jammy\n</code></pre>"},{"location":"0_getting_started/#macos-with-xquartz","title":"macOS (with XQuartz)","text":"<p>1) Install + start XQuartz, enable Allow connections from network clients. 2) In Terminal: <pre><code>xhost + 127.0.0.1\ndocker run -it --rm   --name ros2course   -e DISPLAY=host.docker.internal:0   -e QT_X11_NO_MITSHM=1   -v $(pwd):/home/ros/ros2_tutorial   ambarishgk007/ros2-humble-rviz-gazebo:jammy\n</code></pre></p>"},{"location":"0_getting_started/#new-terminal-for-the-same-container","title":"New terminal for the same container","text":"<p>On the host: <pre><code>docker exec -it ros2course bash\n</code></pre></p>"},{"location":"0_getting_started/#sanity-checks-inside-the-container","title":"Sanity checks inside the container","text":"<pre><code>source /opt/ros/humble/setup.bash\nrviz2                # GUI should open\nros2 --help\n</code></pre>"},{"location":"0_getting_started/#why-this-matters","title":"Why this matters","text":"<p>Containers eliminate 'works on my machine' issues and give you identical ROS 2 + GUI tools anywhere.</p>"},{"location":"0_getting_started/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>You\u2019ll do all tutorials inside this container. Use <code>docker exec</code> whenever you need another terminal.</p>"},{"location":"0_getting_started/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You can pull and run the course image on any OS.</li> <li>You know how to attach more terminals to the same container.</li> <li>You verified GUI support (RViz).</li> </ul> <p>Next: Go to Tutorial 1 - Nodes, Topics, and Services</p>"},{"location":"10_record_and_replay_rosbag2/","title":"Tutorial 10: Record and Replay with rosbag2","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"10_record_and_replay_rosbag2/#concept-overview","title":"Concept Overview","text":"<p>rosbag2 records topics to disk for debugging, datasets, and reproducibility.</p>"},{"location":"10_record_and_replay_rosbag2/#record","title":"Record","text":"<pre><code>ros2 bag record -o my_bag /tf /tf_static /scan /cmd_vel /odom\n</code></pre>"},{"location":"10_record_and_replay_rosbag2/#replay","title":"Replay","text":"<pre><code>ros2 bag play my_bag\n</code></pre>"},{"location":"10_record_and_replay_rosbag2/#why-this-matters","title":"Why this matters","text":"<p>Time\u2011shifting lets you debug without rerunning the robot.</p>"},{"location":"10_record_and_replay_rosbag2/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>You\u2019ll capture scenarios and replay them to test your code deterministically.</p>"},{"location":"10_record_and_replay_rosbag2/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You recorded multiple topics to a bag.</li> <li>You replayed data to reproduce scenarios.</li> <li>You can now iterate faster on algorithms.</li> </ul> <p>Next: Tutorial 11 - RQT, RViz, TF Debugging</p>"},{"location":"11_rqt_rviz_tf_debugging/","title":"Tutorial 11: RQT, RViz, and TF Debugging","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"11_rqt_rviz_tf_debugging/#concept-overview","title":"Concept Overview","text":"<p>Use RQT GUIs and RViz to diagnose graphs, plots, logs, and transforms quickly.</p>"},{"location":"11_rqt_rviz_tf_debugging/#tools","title":"Tools","text":"<pre><code>rqt_graph\nrqt_plot\nros2 run tf2_tools view_frames\nrviz2\n</code></pre>"},{"location":"11_rqt_rviz_tf_debugging/#tips","title":"Tips","text":"<ul> <li><code>rqt_graph</code> shows node\u2013topic connections.</li> <li><code>rqt_plot</code> plots topic values over time.</li> <li><code>view_frames</code> generates a TF tree pdf.</li> </ul>"},{"location":"11_rqt_rviz_tf_debugging/#why-this-matters","title":"Why this matters","text":"<p>Visual tools reduce guesswork and expose misconfigurations.</p>"},{"location":"11_rqt_rviz_tf_debugging/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>You\u2019ll use these constantly when integrating teams\u2019 nodes and params.</p>"},{"location":"11_rqt_rviz_tf_debugging/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You can inspect connections and signals.</li> <li>You can visualize TF trees and displays.</li> <li>You can spot mismatched topics quickly.</li> </ul> <p>Next: Tutorial 12 - Custom Interfaces &amp; Lifecycle</p>"},{"location":"12_custom_interfaces_and_lifecycle/","title":"Tutorial 12: Custom Interfaces and Lifecycle Nodes","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"12_custom_interfaces_and_lifecycle/#concept-overview","title":"Concept Overview","text":"<p>Custom messages/services define your robot\u2019s APIs. Lifecycle nodes formalize startup/teardown states (configure \u2192 activate \u2192 deactivate).</p>"},{"location":"12_custom_interfaces_and_lifecycle/#custom-message","title":"Custom message","text":"<p>Create package <code>my_interfaces</code>, add <code>msg/Num.msg</code>: <pre><code>int64 num\n</code></pre> Build and use it in a pub/sub node.</p>"},{"location":"12_custom_interfaces_and_lifecycle/#lifecycle-flow","title":"Lifecycle flow","text":"<pre><code>ros2 run lifecycle demos_lifecycle_talker\nros2 lifecycle list /talker\nros2 lifecycle set /talker configure\nros2 lifecycle set /talker activate\n</code></pre>"},{"location":"12_custom_interfaces_and_lifecycle/#why-this-matters","title":"Why this matters","text":"<p>Clean, explicit interfaces make systems reusable; lifecycle makes startup predictable.</p>"},{"location":"12_custom_interfaces_and_lifecycle/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>Define messages once, reuse everywhere; gate publishers/subscribers with lifecycle state.</p>"},{"location":"12_custom_interfaces_and_lifecycle/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You created a custom interface.</li> <li>You practiced lifecycle transitions.</li> <li>You prepared nodes for real deployments.</li> </ul> <p>Next: Tutorial 13 - Multi\u2011Robot &amp; Networking</p>"},{"location":"13_multi_robot_and_networking/","title":"Tutorial 13: Multi\u2011Robot and Networking","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"13_multi_robot_and_networking/#concept-overview","title":"Concept Overview","text":"<p>Multi\u2011robot work needs unique namespaces, discovery settings, and sometimes separate domains.</p>"},{"location":"13_multi_robot_and_networking/#namespaces","title":"Namespaces","text":"<pre><code>ros2 run gazebo_ros spawn_entity.py -file /home/ros/ros2_tutorial/examples/simple_robot.urdf -entity robot1 --ros-args -r __ns:=/robot1\nros2 run gazebo_ros spawn_entity.py -file /home/ros/ros2_tutorial/examples/simple_robot.urdf -entity robot2 --ros-args -r __ns:=/robot2\n</code></pre>"},{"location":"13_multi_robot_and_networking/#domains","title":"Domains","text":"<pre><code>export ROS_DOMAIN_ID=7\n</code></pre>"},{"location":"13_multi_robot_and_networking/#test","title":"Test","text":"<pre><code>ros2 topic echo /robot1/scan\nros2 topic echo /robot2/scan\n</code></pre>"},{"location":"13_multi_robot_and_networking/#why-this-matters","title":"Why this matters","text":"<p>Avoid topic collisions and control who talks to whom across networks.</p>"},{"location":"13_multi_robot_and_networking/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>Use namespaces and domains to isolate or connect robots in labs and competitions.</p>"},{"location":"13_multi_robot_and_networking/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You ran multiple robots with namespaces.</li> <li>You controlled discovery with domains.</li> <li>You validated cross\u2011robot topics.</li> </ul> <p>Next: You\u2019ve completed the course!</p>"},{"location":"1_nodes_topics_services/","title":"Tutorial 1: Nodes, Topics, and Services (with Turtlesim)","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"1_nodes_topics_services/#concept-overview","title":"Concept Overview","text":"<p>A node is a running process. Nodes communicate using topics (pub/sub) and services (request/response). This separation lets you build modular robots where sensors, control, and UI are independent.</p>"},{"location":"1_nodes_topics_services/#run-a-node-turtlesim","title":"Run a node (turtlesim)","text":"<p><pre><code>ros2 run turtlesim turtlesim_node\n</code></pre> Open a new terminal and list nodes: <pre><code>ros2 node list\n</code></pre></p>"},{"location":"1_nodes_topics_services/#teleop-node","title":"Teleop node","text":"<p><pre><code>ros2 run turtlesim turtle_teleop_key\n</code></pre> Move the turtle with arrow keys.</p>"},{"location":"1_nodes_topics_services/#command-breakdown","title":"Command Breakdown","text":"<ul> <li><code>ros2 run PACKAGE EXECUTABLE</code> starts a node.</li> <li><code>ros2 node list</code> shows running nodes.</li> <li>Two nodes are now running and communicating indirectly.</li> </ul>"},{"location":"1_nodes_topics_services/#observe-topics-services","title":"Observe topics &amp; services","text":"<pre><code>ros2 topic list\nros2 topic echo /turtle1/pose\nros2 service list\nros2 service call /spawn turtlesim/srv/Spawn \"{x: 2, y: 2, theta: 0.2, name: 't2'}\"\n</code></pre>"},{"location":"1_nodes_topics_services/#why-this-matters","title":"Why this matters","text":"<p>Topics decouple producers and consumers; services handle point-in-time operations (like spawning objects).</p>"},{"location":"1_nodes_topics_services/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>Nearly every sensor publishes on a topic (e.g., <code>/scan</code>). Your code will subscribe to those or provide services.</p>"},{"location":"1_nodes_topics_services/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You ran two nodes and saw them interact.</li> <li>You inspected topics and called a service.</li> <li>You understand node boundaries.</li> </ul> <p>Next: Tutorial 2 - Parameters and Launch Files</p>"},{"location":"2_parameters_and_launchfiles/","title":"Tutorial 2: Parameters and Launch Files","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"2_parameters_and_launchfiles/#concept-overview","title":"Concept Overview","text":"<p>Parameters are node settings you can change without recompiling. Launch files start many nodes with configs in one command.</p>"},{"location":"2_parameters_and_launchfiles/#parameters","title":"Parameters","text":"<pre><code>ros2 param list\nros2 param set /turtlesim background_r 255\nros2 param set /turtlesim background_g 100\n</code></pre>"},{"location":"2_parameters_and_launchfiles/#launch-multiple-nodes","title":"Launch multiple nodes","text":"<p>Use the provided example: <pre><code>ros2 launch /home/ros/ros2_tutorial/examples/turtlesim_start.launch.py\n</code></pre></p>"},{"location":"2_parameters_and_launchfiles/#command-breakdown","title":"Command Breakdown","text":"<ul> <li><code>ros2 param</code> lets you inspect/change runtime settings.</li> <li><code>ros2 launch</code> runs multi-node systems with parameters and remappings.</li> </ul>"},{"location":"2_parameters_and_launchfiles/#why-this-matters","title":"Why this matters","text":"<p>Real robots run many nodes; launch files give you a reproducible recipe.</p>"},{"location":"2_parameters_and_launchfiles/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>You\u2019ll bundle your systems into launch files to start everything consistently on any machine.</p>"},{"location":"2_parameters_and_launchfiles/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You tuned parameters live.</li> <li>You launched multiple nodes at once.</li> <li>You learned how to scale beyond one terminal per node.</li> </ul> <p>Next: Tutorial 3 - Create a Package and Pub/Sub</p>"},{"location":"3_create_package_pubsub/","title":"Tutorial 3: Create a Package and Pub/Sub Nodes (Python)","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"3_create_package_pubsub/#concept-overview","title":"Concept Overview","text":"<p>Packages group code, configs, and launch files. The publisher\u2013subscriber pattern is the core of ROS 2 messaging.</p>"},{"location":"3_create_package_pubsub/#create-and-build","title":"Create and build","text":"<pre><code>mkdir -p ~/ros2_ws/src\ncp -r /home/ros/ros2_tutorial/examples/my_first_pkg ~/ros2_ws/src/\ncd ~/ros2_ws\ncolcon build\nsource install/setup.bash\n</code></pre>"},{"location":"3_create_package_pubsub/#run-them","title":"Run them","text":"<p>Terminal A: <pre><code>ros2 run my_first_pkg publisher_member_function\n</code></pre> Terminal B: <pre><code>ros2 run my_first_pkg subscriber_member_function\n</code></pre></p>"},{"location":"3_create_package_pubsub/#code-walkthrough","title":"Code walkthrough","text":"<ul> <li><code>create_publisher(String, 'chatter', 10)</code>: topic type, name, queue.</li> <li><code>create_timer(0.5, ...)</code>: publish every 0.5s.</li> <li><code>create_subscription(..., 'chatter', ...)</code>: receives messages and logs them.</li> </ul>"},{"location":"3_create_package_pubsub/#why-this-matters","title":"Why this matters","text":"<p>Pub/sub lets you connect many producers and consumers without tight coupling.</p>"},{"location":"3_create_package_pubsub/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>Most of your stack (sensors, planners, controllers) communicates with pub/sub.</p>"},{"location":"3_create_package_pubsub/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You built a package and ran custom nodes.</li> <li>You understood basic rclpy APIs.</li> <li>You saw messages flow over a topic.</li> </ul> <p>Next: Tutorial 4 - Services and Actions</p>"},{"location":"4_services_and_actions/","title":"Tutorial 4: Services and Actions","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"4_services_and_actions/#concept-overview","title":"Concept Overview","text":"<p>Services are synchronous request/response (like a function call). Actions are for long\u2011running goals with feedback (like navigation).</p>"},{"location":"4_services_and_actions/#service-example","title":"Service example","text":"<p>Terminal A: <pre><code>python3 /home/ros/ros2_tutorial/examples/add_two_ints_server.py\n</code></pre> Terminal B: <pre><code>python3 /home/ros/ros2_tutorial/examples/add_two_ints_client.py\n</code></pre></p>"},{"location":"4_services_and_actions/#code-walkthrough","title":"Code walkthrough","text":"<ul> <li>Server: <code>create_service(AddTwoInts, 'add_two_ints', callback)</code></li> <li>Client: waits for service, sends request, prints result.</li> </ul>"},{"location":"4_services_and_actions/#actions-explore","title":"Actions (explore)","text":"<pre><code>ros2 action list\nros2 action info /turtle1/rotate_absolute\n</code></pre>"},{"location":"4_services_and_actions/#why-this-matters","title":"Why this matters","text":"<p>Some tasks need a clear request/response or progress feedback (e.g., move to pose).</p>"},{"location":"4_services_and_actions/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>Use services to configure or trigger events; use actions for goals that take time.</p>"},{"location":"4_services_and_actions/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You implemented a service server and client.</li> <li>You learned when to use actions vs services.</li> <li>You explored turtlesim\u2019s action interface.</li> </ul> <p>Next: Tutorial 5 - TF2 and RViz2</p>"},{"location":"5_tf2_and_rviz2/","title":"Tutorial 5: TF2 and RViz2 Visualization","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"5_tf2_and_rviz2/#concept-overview","title":"Concept Overview","text":"<p>TF2 tracks coordinate frames so nodes agree on where things are. RViz2 lets you visualize data and frames.</p>"},{"location":"5_tf2_and_rviz2/#launch-demo","title":"Launch demo","text":"<p><pre><code>ros2 launch /home/ros/ros2_tutorial/examples/tf_demo.launch.py\n</code></pre> Then open RViz2 and add the TF display.</p>"},{"location":"5_tf2_and_rviz2/#inspect-transforms","title":"Inspect transforms","text":"<pre><code>ros2 run tf2_ros tf2_echo base_link camera_link\n</code></pre>"},{"location":"5_tf2_and_rviz2/#why-this-matters","title":"Why this matters","text":"<p>Navigation, manipulation, and perception all depend on consistent frames.</p>"},{"location":"5_tf2_and_rviz2/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>You\u2019ll broadcast static and dynamic transforms so all nodes share a common spatial view.</p>"},{"location":"5_tf2_and_rviz2/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You launched a TF demo and viewed frames.</li> <li>You inspected a transform on the CLI.</li> <li>You used RViz2 for visualization.</li> </ul> <p>Next: Tutorial 6 - URDF and Robot Description</p>"},{"location":"6_urdf_robot_description/","title":"Tutorial 6: URDF and Robot Description","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"6_urdf_robot_description/#concept-overview","title":"Concept Overview","text":"<p>URDF (XML) describes your robot\u2019s links and joints. This is the model that RViz/Gazebo and many tools consume.</p>"},{"location":"6_urdf_robot_description/#inspect-the-example","title":"Inspect the example","text":"<p>Open: <pre><code>/home/ros/ros2_tutorial/examples/simple_robot.urdf\n</code></pre></p>"},{"location":"6_urdf_robot_description/#visualize-in-rviz2","title":"Visualize in RViz2","text":"<pre><code>ros2 launch urdf_tutorial display.launch.py model:=/home/ros/ros2_tutorial/examples/simple_robot.urdf\n</code></pre>"},{"location":"6_urdf_robot_description/#why-this-matters","title":"Why this matters","text":"<p>A robot description is the single source of truth for geometry and kinematics.</p>"},{"location":"6_urdf_robot_description/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>You\u2019ll add sensors/links here and reuse it in simulation and real robots.</p>"},{"location":"6_urdf_robot_description/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You learned what URDF is and opened a sample model.</li> <li>You viewed the model in RViz2.</li> <li>You\u2019re ready to take it into a simulator.</li> </ul> <p>Next: Tutorial 7 - Gazebo: Empty World &amp; Spawn</p>"},{"location":"7_gazebo_empty_world_and_spawn/","title":"Tutorial 7: Gazebo - Empty World &amp; Spawn","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"7_gazebo_empty_world_and_spawn/#concept-overview","title":"Concept Overview","text":"<p>Gazebo (classic) simulates physics and sensors so you can test without hardware.</p>"},{"location":"7_gazebo_empty_world_and_spawn/#start-an-empty-world","title":"Start an empty world","text":"<pre><code>ros2 launch /home/ros/ros2_tutorial/examples/empty_world.launch.py\n</code></pre>"},{"location":"7_gazebo_empty_world_and_spawn/#spawn-your-urdf","title":"Spawn your URDF","text":"<pre><code>ros2 run gazebo_ros spawn_entity.py   -file /home/ros/ros2_tutorial/examples/simple_robot.urdf   -entity simple_bot\n</code></pre>"},{"location":"7_gazebo_empty_world_and_spawn/#verify-topics","title":"Verify topics","text":"<pre><code>ros2 topic list\n</code></pre>"},{"location":"7_gazebo_empty_world_and_spawn/#why-this-matters","title":"Why this matters","text":"<p>Simulation lets you iterate quickly and safely.</p>"},{"location":"7_gazebo_empty_world_and_spawn/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>You\u2019ll spawn and respawn robots while developing perception and control.</p>"},{"location":"7_gazebo_empty_world_and_spawn/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You ran Gazebo and spawned your robot.</li> <li>You saw simulator topics appear in ROS 2.</li> <li>You can change spawn poses and respawn.</li> </ul> <p>Next: Tutorial 8 - Gazebo: Sensors and Plugins</p>"},{"location":"8_gazebo_sensors_and_plugins/","title":"Tutorial 8: Gazebo - Sensors and Plugins","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"8_gazebo_sensors_and_plugins/#concept-overview","title":"Concept Overview","text":"<p>Gazebo plugins attach simulated sensors/actuators to links. They produce ROS topics just like the real hardware drivers.</p>"},{"location":"8_gazebo_sensors_and_plugins/#add-a-lidar","title":"Add a LiDAR","text":"<p>See: <pre><code>/home/ros/ros2_tutorial/examples/simple_robot_with_sensors.urdf\n</code></pre> Spawn: <pre><code>ros2 run gazebo_ros spawn_entity.py   -file /home/ros/ros2_tutorial/examples/simple_robot_with_sensors.urdf   -entity sensor_bot\n</code></pre></p>"},{"location":"8_gazebo_sensors_and_plugins/#watch-the-data","title":"Watch the data","text":"<pre><code>ros2 topic list | grep scan\nros2 topic echo /scan\n</code></pre>"},{"location":"8_gazebo_sensors_and_plugins/#why-this-matters","title":"Why this matters","text":"<p>Sensors are the inputs to autonomy; you\u2019ll test algorithms before touching real hardware.</p>"},{"location":"8_gazebo_sensors_and_plugins/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>You\u2019ll publish scans/images/IMU data from Gazebo and consume them in your nodes.</p>"},{"location":"8_gazebo_sensors_and_plugins/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You added a LiDAR plugin and viewed <code>/scan</code>.</li> <li>You understand plugin-produced topics.</li> <li>You can extend the URDF with more sensors.</li> </ul> <p>Next: Tutorial 9 - Navigation (Nav2)</p>"},{"location":"9_navigation_nav2/","title":"Tutorial 9: Navigation (Nav2) in Simulation","text":"<p>Environment Note - Make sure the course container is running (see Tutorial 0). - Open a new terminal in the same container with: <code>docker exec -it ros2course bash</code> - In each new terminal: <code>source /opt/ros/humble/setup.bash</code></p>"},{"location":"9_navigation_nav2/#concept-overview","title":"Concept Overview","text":"<p>Nav2 moves a robot to a goal using mapping, planning, and control. It consumes sensor topics and TF.</p>"},{"location":"9_navigation_nav2/#launch-nav2-with-sim-time","title":"Launch Nav2 (with sim time)","text":"<p><pre><code>ros2 launch nav2_bringup bringup_launch.py use_sim_time:=True\n</code></pre> Send a goal: <pre><code>ros2 action send_goal /navigate_to_pose nav2_msgs/action/NavigateToPose \"{pose: {pose: {position: {x: 1.0, y: 0.5, z: 0.0}, orientation: {w: 1.0}}}}\"\n</code></pre></p>"},{"location":"9_navigation_nav2/#why-this-matters","title":"Why this matters","text":"<p>Autonomy relies on accurate transforms and sensor data to plan safe paths.</p>"},{"location":"9_navigation_nav2/#how-youll-use-it","title":"How you\u2019ll use it","text":"<p>You\u2019ll integrate your URDF + Gazebo sensors with Nav2 for closed-loop navigation.</p>"},{"location":"9_navigation_nav2/#wrapup","title":"Wrap\u2011up","text":"<ul> <li>You started Nav2 and sent a goal.</li> <li>You saw how actions control long-running tasks.</li> <li>You integrated sim time with the stack.</li> </ul> <p>Next: Tutorial 10 - rosbag2</p>"},{"location":"CHEATSHEET/","title":"\ud83e\udded ROS 2 + Docker Cheatsheet (Humble / Jammy)","text":"<p>A quick reference for running, managing, and testing your ROS 2 environment inside the <code>ambarishgk007/ros2-humble-rviz-gazebo:jammy</code> Docker image.</p>"},{"location":"CHEATSHEET/#docker-commands","title":"\ud83d\udc33 Docker Commands","text":""},{"location":"CHEATSHEET/#pull-run-the-container","title":"\ud83d\udd39 Pull &amp; Run the Container","text":"<pre><code># Pull the prebuilt image (multi-arch: amd64 + arm64)\ndocker pull ambarishgk007/ros2-humble-rviz-gazebo:jammy\n\n# Run interactively (Linux example)\nxhost +local:\ndocker run -it --rm --name ros2course   -e DISPLAY=$DISPLAY -e QT_X11_NO_MITSHM=1   -v /tmp/.X11-unix:/tmp/.X11-unix:ro   -v $(pwd):/home/ros/ros2_tutorial   --device /dev/dri   ambarishgk007/ros2-humble-rviz-gazebo:jammy\n</code></pre>"},{"location":"CHEATSHEET/#container-management","title":"\ud83d\udd39 Container Management","text":"<pre><code>docker ps                    # list running containers\ndocker exec -it ros2course bash   # open a new terminal inside the same container\ndocker stop ros2course       # stop container manually\ndocker rm ros2course         # remove stopped container\ndocker images                # list downloaded images\ndocker rmi &lt;image_id&gt;        # remove an image\n</code></pre>"},{"location":"CHEATSHEET/#build-your-own-optional","title":"\ud83d\udd39 Build Your Own (optional)","text":"<pre><code>docker build -t my-ros2-image:jammy -f docker/Dockerfile .\n</code></pre>"},{"location":"CHEATSHEET/#inside-the-container","title":"\ud83e\udde0 Inside the Container","text":""},{"location":"CHEATSHEET/#environment-setup","title":"\ud83d\udd39 Environment Setup","text":"<pre><code>source /opt/ros/humble/setup.bash\n</code></pre> <p>\u2705 Tip: In this image, the ROS 2 environment is already auto-sourced in <code>.bashrc</code>.</p>"},{"location":"CHEATSHEET/#quick-sanity-tests","title":"\ud83d\udd39 Quick Sanity Tests","text":"<pre><code>rviz2               # Launch RViz2 GUI\ngazebo              # Launch Gazebo Classic\nxeyes               # (tiny X11 test window)\nglxinfo -B          # Check OpenGL renderer info\n</code></pre>"},{"location":"CHEATSHEET/#ros-2-core-commands","title":"\ud83d\ude80 ROS 2 Core Commands","text":""},{"location":"CHEATSHEET/#nodes-topics-services","title":"\ud83d\udd39 Nodes, Topics, Services","text":"<pre><code>ros2 node list\nros2 topic list\nros2 service list\nros2 action list\n</code></pre>"},{"location":"CHEATSHEET/#inspecting-data","title":"\ud83d\udd39 Inspecting Data","text":"<pre><code>ros2 topic echo /topic_name\nros2 topic info /topic_name\nros2 interface show geometry_msgs/msg/Twist\n</code></pre>"},{"location":"CHEATSHEET/#launching-demos","title":"\ud83d\udd39 Launching Demos","text":"<pre><code>ros2 run demo_nodes_cpp talker\nros2 run demo_nodes_cpp listener\nros2 launch demo_nodes_cpp talk_listen.launch.py\n</code></pre>"},{"location":"CHEATSHEET/#building-and-packages","title":"\ud83e\udde9 Building and Packages","text":""},{"location":"CHEATSHEET/#create-a-workspace","title":"\ud83d\udd39 Create a Workspace","text":"<pre><code>mkdir -p ~/ros2_ws/src\ncd ~/ros2_ws/src\nros2 pkg create --build-type ament_python my_pkg\n</code></pre>"},{"location":"CHEATSHEET/#build-source","title":"\ud83d\udd39 Build &amp; Source","text":"<pre><code>cd ~/ros2_ws\ncolcon build\nsource install/setup.bash\n</code></pre>"},{"location":"CHEATSHEET/#parameters-bags-and-tf","title":"\ud83e\uddfe Parameters, Bags, and TF","text":""},{"location":"CHEATSHEET/#parameters","title":"\ud83d\udd39 Parameters","text":"<pre><code>ros2 param list\nros2 param get /node_name param_name\nros2 param set /node_name param_name value\n</code></pre>"},{"location":"CHEATSHEET/#rosbag2-recording","title":"\ud83d\udd39 rosbag2 Recording","text":"<pre><code>ros2 bag record -a                      # record all topics\nros2 bag record /topic1 /topic2         # record selected topics\nros2 bag info &lt;bag_name&gt;\nros2 bag play &lt;bag_name&gt;\n</code></pre>"},{"location":"CHEATSHEET/#tf2-utilities","title":"\ud83d\udd39 TF2 Utilities","text":"<pre><code>ros2 run tf2_tools view_frames          # generate TF tree PDF\nros2 run tf2_ros tf2_echo base_link camera_link\n</code></pre>"},{"location":"CHEATSHEET/#useful-shortcuts","title":"\ud83e\uddf0 Useful Shortcuts","text":"Task Command Open new terminal in same container <code>docker exec -it ros2course bash</code> Save container state as image <code>docker commit ros2course my-snapshot:latest</code> Clean all stopped containers <code>docker container prune</code> Clean dangling images <code>docker image prune</code>"},{"location":"CHEATSHEET/#troubleshooting","title":"\u26a1 Troubleshooting","text":"Problem Solution GUI apps not opening (Linux) Run <code>xhost +local:</code> before <code>docker run</code> GUI apps not opening (Windows WSL2) Run container inside WSL; check <code>/mnt/wslg</code> exists macOS blank window Ensure XQuartz is running (<code>xhost + 127.0.0.1</code>) Permission denied (Docker) Add user to Docker group \u2192 <code>sudo usermod -aG docker $USER</code> <p>Author: Ambarish G.K Docker Hub: ambarishgk007/ros2-humble-rviz-gazebo</p>"},{"location":"README_mmdetection3d/","title":"MMDetection3D LiDAR Demo (Docker)","text":"<p>This tutorial shows how to run a PointPillars LiDAR object-detection demo from MMDetection3D using a prebuilt Docker image, with GUI display support on Linux, macOS, and Windows.</p> <p>Docker image</p> <p><code>docker pull ambarishgk007/mmdetect-cuda118:latest</code> (CUDA 11.8, Torch 2.1.2, mmcv 2.1.0, mmdet 3.2.0, mmdet3d 1.4.0, NumPy 1.26.4)</p>"},{"location":"README_mmdetection3d/#1-enable-gui-so-show-opens-a-window","title":"1) Enable GUI (so <code>--show</code> opens a window)","text":"<p>You can run headless (skip these steps and omit <code>--show</code>) or enable GUI per OS.</p>"},{"location":"README_mmdetection3d/#linux-x11-ubuntu-desktop","title":"Linux (X11; Ubuntu desktop)","text":"<p><pre><code># allow the container to access your X server\nxhost +local:root\n</code></pre> If you use Wayland, ensure XWayland is installed and consider adding <code>-e QT_X11_NO_MITSHM=1</code> in the <code>docker run</code> command.</p>"},{"location":"README_mmdetection3d/#macos-docker-desktop-xquartz","title":"macOS (Docker Desktop + XQuartz)","text":"<p>1) Install XQuartz, open Preferences \u2192 Security and enable Allow connections from network clients. 2) Start XQuartz, then: <pre><code>xhost + 127.0.0.1\n</code></pre> We will use <code>DISPLAY=host.docker.internal:0</code> in the <code>docker run</code> command.</p>"},{"location":"README_mmdetection3d/#windows-docker-desktop-vcxsrv","title":"Windows (Docker Desktop + VcXsrv)","text":"<p>1) Install and launch VcXsrv (Multi-window or One large window, Disable access control). 2) We will use <code>DISPLAY=host.docker.internal:0.0</code> in the <code>docker run</code> command.</p>"},{"location":"README_mmdetection3d/#2-run-the-container","title":"2) Run the container","text":""},{"location":"README_mmdetection3d/#linux-x11","title":"Linux (X11)","text":"<pre><code>docker run --name mmdetection3d-1 \\\n  --gpus all \\\n  --shm-size=8g \\\n  -e DISPLAY=$DISPLAY \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix:rw \\\n  -v \"$PWD\":/workspace \\\n  -w /workspace \\\n  -it ambarishgk007/mmdetect-cuda118:latest bash\n</code></pre> <p>If you need ROS\u00a02 local discovery across the host network, you may add <code>--network host</code> (Linux only).</p>"},{"location":"README_mmdetection3d/#macos","title":"macOS","text":"<pre><code>docker run --name mmdetection3d-1 \\\n  --gpus all \\\n  --shm-size=8g \\\n  -e DISPLAY=host.docker.internal:0 \\\n  -e QT_X11_NO_MITSHM=1 \\\n  -v \"$PWD\":/workspace \\\n  -w /workspace \\\n  -it ambarishgk007/mmdetect-cuda118:latest bash\n</code></pre>"},{"location":"README_mmdetection3d/#windows-powershell","title":"Windows (PowerShell)","text":"<pre><code>docker run --name mmdetection3d-1 `\n  --gpus all `\n  --shm-size=8g `\n  -e DISPLAY=host.docker.internal:0.0 `\n  -e QT_X11_NO_MITSHM=1 `\n  -v \"%cd%\":/workspace `\n  -w /workspace `\n  -it ambarishgk007/mmdetect-cuda118:latest bash\n</code></pre>"},{"location":"README_mmdetection3d/#3-run-the-mmdetection3d-demo","title":"3) Run the MMDetection3D demo","text":"<p>Inside the container: <pre><code>cd /workspace/mmdetection3d\n\n# Show a window (if GUI is configured) and also save outputs/\npython demo/pcd_demo.py \\\n  demo/data/kitti/000008.bin \\\n  checkpoints/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py \\\n  checkpoints/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth \\\n  --device cuda:0 \\\n  --pred-score-thr 0.3 \\\n  --out-dir outputs \\\n  --show\n</code></pre></p> <ul> <li>Headless? Remove <code>--show</code>. Images/meshes will still be saved in <code>outputs/</code>.</li> <li>You can try other configs in <code>configs/</code> and compatible checkpoints in <code>checkpoints/</code>.</li> </ul>"},{"location":"README_mmdetection3d/#4-troubleshooting","title":"4) Troubleshooting","text":"<ul> <li>No window appears</li> <li>Linux: ensure <code>xhost +local:root</code> and the <code>/tmp/.X11-unix</code> bind mount are present.</li> <li>macOS: confirm XQuartz is running and <code>DISPLAY=host.docker.internal:0</code> was set.</li> <li> <p>Windows: ensure VcXsrv is running with Disable access control; use <code>DISPLAY=host.docker.internal:0.0</code>.</p> </li> <li> <p>NumPy ABI error mentioning \u201cNumPy 2.x\u201d   The image pins NumPy to 1.26.4. If it was upgraded by another package, reset it:   <pre><code>python -m pip install --no-cache-dir \"numpy==1.26.4\"\n</code></pre></p> </li> <li> <p>OpenCV GL errors   The image ships <code>opencv-python-headless</code>. To use full OpenCV GUI instead, install system GL inside the container:   <pre><code>apt-get update &amp;&amp; apt-get install -y libgl1 libglib2.0-0\n</code></pre></p> </li> <li> <p>MMCV CUDA ops error (<code>mmcv._ext</code>)   The image already contains a matching wheel for cu118. If you change PyTorch or CUDA, reinstall MMCV with the proper prebuilt wheel for your new Torch/CUDA pair.</p> </li> </ul>"},{"location":"README_mmdetection3d/#5-clean-up-re-enter","title":"5) Clean up / re-enter","text":"<pre><code># leave\nexit\n\n# re-enter same container\ndocker start -ai mmdetection3d-1\n\n# delete it when done\ndocker rm -f mmdetection3d-1\n</code></pre>"},{"location":"README_ros2_mmdetect_container/","title":"ROS 2 + MMDetection3D LiDAR Detection Container","text":"<p>This container bundles ROS 2 Humble and MMDetection3D (PointPillars) for real\u2011time 3D object detection from LiDAR data. It subscribes to <code>/velodyne_points</code>, runs inference on the GPU, and publishes 3D boxes for RViz 2.</p>"},{"location":"README_ros2_mmdetect_container/#image","title":"\ud83e\uddf1 Image","text":"<p>Docker Hub: <code>ambarishgk007/ros2-mmdetect-cuda118:latest</code></p> <p>Stack | Component       | Version | |-----------------|---------| | Ubuntu          | 22.04 LTS | | CUDA            | 11.8 (devel, with <code>nvcc</code>) | | PyTorch         | 2.1.2 + cu118 | | MMCV            | 2.1.0 | | MMDetection     | 3.2.0 | | MMDetection3D   | 1.4.0 | | NumPy           | 1.26.4 | | ROS 2           | Humble (desktop + perception) |</p> <p>The image clones your repo during build to: <code>/workspace/Nvidia-Isaac-ROS</code> and includes MMDetection3D checkpoints at: <code>/workspace/mmdetection3d/checkpoints</code>.</p>"},{"location":"README_ros2_mmdetect_container/#pull","title":"\u2b07\ufe0f Pull","text":"<pre><code>docker pull ambarishgk007/ros2-mmdetect-cuda118:latest\n</code></pre>"},{"location":"README_ros2_mmdetect_container/#run-the-container","title":"\ud83d\ude80 Run the Container","text":""},{"location":"README_ros2_mmdetect_container/#linux-x11-gui","title":"Linux (X11 GUI)","text":"<pre><code>xhost +local:root\ndocker run --name ros2-mmdetection3d   --gpus all   --network host   --shm-size=8g   -e DISPLAY=$DISPLAY   -v /tmp/.X11-unix:/tmp/.X11-unix:rw   -v \"$PWD\":/workspace   -w /workspace   -it ambarishgk007/ros2-mmdetect-cuda118:latest bash\n</code></pre>"},{"location":"README_ros2_mmdetect_container/#macos-xquartz-windows-vcxsrv","title":"macOS (XQuartz) / Windows (VcXsrv)","text":"<p>Run an X server, then set DISPLAY: <pre><code># macOS\n-e DISPLAY=host.docker.internal:0\n# Windows\n-e DISPLAY=host.docker.internal:0.0\n</code></pre></p>"},{"location":"README_ros2_mmdetect_container/#whats-inside","title":"\ud83e\udde0 What\u2019s Inside","text":"<ol> <li> <p>ROS 2 &amp; GPU ready <pre><code>source /opt/ros/humble/setup.bash\nnvidia-smi\nros2 --version\n</code></pre></p> </li> <li> <p>Repo &amp; scripts <pre><code>/workspace/Nvidia-Isaac-ROS/Examples/\n  \u251c\u2500\u2500 real.py                              # ROS 2 node (recommended)\n  \u2514\u2500\u2500 realtimemmdetectboundingbox.py       # legacy node variant\n</code></pre></p> </li> <li> <p>Models <pre><code>/workspace/mmdetection3d/checkpoints/\n  \u251c\u2500\u2500 pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py\n  \u2514\u2500\u2500 hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth\n</code></pre></p> </li> </ol>"},{"location":"README_ros2_mmdetect_container/#run-the-detection-node","title":"\u25b6\ufe0f Run the Detection Node","text":"<p>1) Source ROS 2 <pre><code>source /opt/ros/humble/setup.bash\n</code></pre></p> <p>2) Start a LiDAR source - Real sensor driver publishing <code>/velodyne_points</code>, or - Replay a rosbag: <pre><code>ros2 bag play your_lidar_rosbag\n</code></pre></p> <p>3) Run the node <pre><code>python3 /workspace/Nvidia-Isaac-ROS/Examples/real.py\n# or\npython3 /workspace/Nvidia-Isaac-ROS/Examples/realtimemmdetectboundingbox.py\n</code></pre> Expected logs: <pre><code>[INFO] [lidar_detection_node]: Loading model on cuda:0\u2026\n[INFO] [lidar_detection_node]: Model ready.\n[INFO] [lidar_detection_node]: Running inference\u2026\n[INFO] [lidar_detection_node]: Inference done.\n</code></pre></p>"},{"location":"README_ros2_mmdetect_container/#visualize-in-rviz-2","title":"\ud83d\udda5\ufe0f Visualize in RViz 2","text":"<p><pre><code>rviz2\n</code></pre> Add displays: | Type        | Topic                      | Description                       | |-------------|----------------------------|-----------------------------------| | Marker      | <code>/bounding_box_edges</code>      | Green line edges of each 3D box   | | MarkerArray | <code>/lidar_bounding_boxes</code>    | Semi\u2011transparent green cubes      |</p> <p>Set Fixed Frame to your LiDAR frame (e.g., <code>velodyne</code>).</p>"},{"location":"README_ros2_mmdetect_container/#how-it-works","title":"\ud83e\udde9 How It Works","text":"<ol> <li>Subscribe to <code>/velodyne_points</code> (<code>sensor_msgs/PointCloud2</code>).  </li> <li>Convert to NumPy <code>(N,4)</code> \u2192 <code>[x, y, z, intensity]</code> (intensity can be zero).  </li> <li>Run MMDetection3D PointPillars inference.  </li> <li>Publish detections as:</li> <li><code>visualization_msgs/Marker</code> on <code>/bounding_box_edges</code> </li> <li><code>visualization_msgs/MarkerArray</code> on <code>/lidar_bounding_boxes</code> </li> <li>RViz renders the 3D boxes in real time.</li> </ol>"},{"location":"README_ros2_mmdetect_container/#troubleshooting","title":"\ud83e\uddf0 Troubleshooting","text":"Symptom Fix <code>FileNotFoundError: pointpillars_...py</code> Use the absolute config/checkpoint paths shown above. <code>No boxes detected</code> Verify cloud density and coordinate frame; test with KITTI-like scans. <code>ModuleNotFoundError: tf_transformations</code> Remove that import or <code>pip install tf-transformations</code>. No markers in RViz Ensure Fixed Frame equals the LiDAR frame; add Marker/MarkerArray displays. NumPy ABI error <code>pip install 'numpy==1.26.4' --force-reinstall</code> inside the container."},{"location":"README_ros2_mmdetect_container/#summary","title":"\ud83e\uddfe Summary","text":"<ul> <li>Image: <code>ambarishgk007/ros2-mmdetect-cuda118:latest</code> </li> <li>Input topic: <code>/velodyne_points</code> (<code>sensor_msgs/PointCloud2</code>)  </li> <li>Output topics: <code>/bounding_box_edges</code> (Marker), <code>/lidar_bounding_boxes</code> (MarkerArray)  </li> <li>Goal: Real\u2011time LiDAR 3D detection (PointPillars) in ROS 2 Humble.</li> </ul>"},{"location":"ROS2_Jetson_Vision_Suite_README/","title":"ROS 2 Jetson Vision Suite","text":""},{"location":"ROS2_Jetson_Vision_Suite_README/#yolov8-2d-mmdetection3d-3d-lidar-core-ros-2-demos","title":"YOLOv8 (2D) + MMDetection3D (3D LiDAR) + Core ROS 2 Demos","text":"<p>All running inside a single GPU-accelerated Jetson container</p> <p>This repository contains a ROS 2 Humble workspace that integrates:</p> Package Description mmdetect_lidar_demo Real-time 3D LiDAR detection using MMDetection3D (PointPillars, SECOND, CenterPoint, etc.) image_demo YOLOv8 2D detection pipeline over images/video ros2demo Basic ROS 2 publisher/subscriber examples <p>All packages are fully tested on Jetson using the container:</p> <pre><code>ambarishgk007/jetson-mmdetetc3d-pth12.2-torch2.1:latest \n</code></pre> <p>Built with: - JetPack 6 / L4T r36.x - CUDA 12.2 - PyTorch 2.1.0 - MMCV 2.1.0 - MMDetection 3.2.0 - MMDetection3D 1.4.0 - YOLOv8 - ROS 2 Humble  </p>"},{"location":"ROS2_Jetson_Vision_Suite_README/#1-running-the-container","title":"1. Running the Container","text":""},{"location":"ROS2_Jetson_Vision_Suite_README/#10-pull-the-image","title":"1.0 Pull the image","text":"<pre><code>sudo docker pull ambarishgk007/ambarishgk007/jetson-ros2-mmdetetc3d-yolo-pth12.2-torch2.1:latest\n</code></pre>"},{"location":"ROS2_Jetson_Vision_Suite_README/#11-start-the-container","title":"1.1 Start the container","text":"<pre><code>xhost +local:root\n\nsudo docker run --name ros2-vision-suite   --runtime nvidia   --gpus all   --network host   --shm-size=8g   -e DISPLAY=$DISPLAY   -v /tmp/.X11-unix:/tmp/.X11-unix:rw   -it ambarishgk007/ambarishgk007/jetson-ros2-mmdetetc3d-yolo-pth12.2-torch2.1:latest  bash\n</code></pre> <p>Inside container:</p> <pre><code>source /opt/ros/humble/setup.bash\ncd /workspace/ROS2-Tutorial\nsource install/setup.bash\n</code></pre>"},{"location":"ROS2_Jetson_Vision_Suite_README/#2-ros-2-workspace-layout","title":"2. ROS 2 Workspace Layout","text":"<pre><code>ROS2-Tutorial/\n  \u251c\u2500\u2500 mmdetect_lidar_demo/\n  \u251c\u2500\u2500 image_demo/\n  \u251c\u2500\u2500 ros2demo/\n  \u251c\u2500\u2500 checkpoints/\n  \u251c\u2500\u2500 examples/data/\n  \u2514\u2500\u2500 install/, build/, log/\n</code></pre>"},{"location":"ROS2_Jetson_Vision_Suite_README/#3-lidar-detection-demo","title":"3. LiDAR Detection Demo","text":"<p>Run rosbag:</p> <pre><code>ros2 bag play multilidarcalib12\n</code></pre> <p>Start detector:</p> <pre><code>ros2 run mmdetect_lidar_demo mmdetect_lidar_demo\n</code></pre> <p>RViz setup:</p> <ul> <li><code>/velodyne_points</code></li> <li><code>/bounding_box_edges</code></li> <li><code>/lidar_bounding_boxes</code></li> </ul>"},{"location":"ROS2_Jetson_Vision_Suite_README/#4-switching-mmdetection3d-models","title":"4. Switching MMDetection3D Models","text":"<p>Example:</p> <pre><code>ros2 run mmdetect_lidar_demo mmdetect_lidar_demo   --ros-args   -p config_file:=/workspace/mmdetection3d/configs/second/...py   -p checkpoint_file:=/workspace/mmdetection3d/checkpoints/...pth\n</code></pre>"},{"location":"ROS2_Jetson_Vision_Suite_README/#5-yolov8-image-demo","title":"5. YOLOv8 Image Demo","text":"<p>Run:</p> <pre><code>ros2 launch image_demo yolo_demo.launch.py\n</code></pre> <p>View in RViz or rqt:</p> <pre><code>rqt_image_view\n</code></pre> <p>Topic: <code>/image_yolo</code></p>"},{"location":"ROS2_Jetson_Vision_Suite_README/#6-basic-ros-2-demo","title":"6. Basic ROS 2 Demo","text":"<p>Publisher:</p> <pre><code>ros2 run ros2demo ros2demo_publisher\n</code></pre> <p>Subscriber:</p> <pre><code>ros2 run ros2demo ros2demo_subscriber\n</code></pre>"},{"location":"ROS2_Jetson_Vision_Suite_README/#7-running-full-perception-stack","title":"7. Running Full Perception Stack","text":"<p>YOLO (2D):</p> <pre><code>ros2 launch image_demo yolo_demo.launch.py\n</code></pre> <p>LiDAR 3D:</p> <pre><code>ros2 run mmdetect_lidar_demo mmdetect_lidar_demo\n</code></pre> <p>Bag:</p> <pre><code>ros2 bag play multilidarcalib12\n</code></pre> <p>RViz:</p> <pre><code>rviz2\n</code></pre> <p>Add displays: - <code>/velodyne_points</code> - <code>/bounding_box_edges</code> - <code>/lidar_bounding_boxes</code> - <code>/image_yolo</code></p>"},{"location":"jetson_mmdet3d_suite/","title":"Jetson MMDetection3D Suite","text":""},{"location":"jetson_mmdet3d_suite/#torch-21-cuda-122-mmdetection3d-140-pointpillars-demo","title":"Torch 2.1 + CUDA 12.2 + MMDetection3D 1.4.0 (PointPillars demo)","text":"<p>All inside a single GPU accelerated container for Jetson Orin (JetPack 6 / L4T r36.x)</p> <p>This document explains how to use the Jetson MMDetection3D container:</p> <p><code>ambarishgk007/jetson-mmdetetc3d-pth12.2-torch2.1:latest</code></p> <p>It provides a ready to use environment for 3D LiDAR detection using OpenMMLab:</p> Component Version / Info Base image <code>dustynv/l4t-ml:r36.2.0</code> CUDA 12.2 PyTorch 2.1.0 (Jetson aarch64 build) NumPy 1.26.4 (kept &lt; 2 for ABI safety) OpenCV 4.8.1.78 (Python) MMCV 2.1.0 (built from source with CUDA) MMEngine &gt;= 0.10.4 MMDetection 3.2.0 MMDetection3D 1.4.0 (dev 1.x branch) Demo PointPillars on KITTI pc bin file"},{"location":"jetson_mmdet3d_suite/#1-pull-and-run-the-container","title":"1. Pull and Run the Container","text":""},{"location":"jetson_mmdet3d_suite/#11-pull","title":"1.1 Pull","text":"<pre><code>docker pull ambarishgk007/jetson-mmdetetc3d-pth12.2-torch2.1:latest\n</code></pre>"},{"location":"jetson_mmdet3d_suite/#12-run","title":"1.2 Run","text":"<pre><code>docker run --name jetson-mmdet3d   --runtime nvidia   --network host   --shm-size=8g   -it ambarishgk007/jetson-mmdetetc3d-pth12.2-torch2.1:latest   bash\n</code></pre>"},{"location":"jetson_mmdet3d_suite/#2-verify-the-environment","title":"2. Verify the Environment","text":"<pre><code>python3 - &lt;&lt; 'EOF'\nimport importlib\n\ndef safe_import(name):\n    try:\n        m = importlib.import_module(name)\n        print(f\"{name}: OK\")\n        return m\n    except Exception as e:\n        print(f\"{name}: FAIL -&gt; {e}\")\n        return None\n\nmods = {}\nfor name in [\"numpy\", \"torch\", \"cv2\", \"mmengine\", \"mmcv\", \"mmdet\", \"mmdet3d\"]:\n    m = safe_import(name)\n    if m is not None:\n        mods[name] = m\n\nprint(\"\\n=== VERSIONS ===\")\nif \"numpy\" in mods:\n    print(\"numpy:\", mods[\"numpy\"].__version__)\nif \"torch\" in mods:\n    print(\"torch:\", mods[\"torch\"].__version__)\n    print(\"cuda version:\", mods[\"torch\"].version.cuda)\n    print(\"cuda available:\", mods[\"torch\"].cuda.is_available())\nif \"cv2\" in mods:\n    print(\"cv2:\", mods[\"cv2\"].__version__)\nif \"mmengine\" in mods:\n    print(\"mmengine:\", mods[\"mmengine\"].__version__)\nif \"mmcv\" in mods:\n    print(\"mmcv:\", mods[\"mmcv\"].__version__)\nif \"mmdet\" in mods:\n    print(\"mmdet:\", mods[\"mmdet\"].__version__)\nif \"mmdet3d\" in mods:\n    print(\"mmdet3d:\", mods[\"mmdet3d\"].__version__)\n\n# mmcv CUDA ops test\nif \"mmcv\" in mods:\n    from mmcv.ops import nms\n    print(\"\\n[OK] mmcv.ops.nms imported\")\n\n# simple torch CUDA test\nif \"torch\" in mods:\n    import torch\n    x = torch.randn(1, 3, 224, 224, device=\"cuda\")\n    y = torch.nn.functional.relu(x)\n    print(\"[OK] torch CUDA tensor op:\", y.shape)\nEOF\n</code></pre>"},{"location":"jetson_mmdet3d_suite/#3-run-mmdetection3d-pointpillars-demo","title":"3. Run MMDetection3D PointPillars Demo","text":"<pre><code>cd /root/mmdetection3d\n\npython3 demo/pcd_demo.py   demo/data/kitti/000008.bin   configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car.py   checkpoints/hv_pointpillars_secfpn_6x8_160e_kitti-3d-car_20220331_134606-d42d15ed.pth   --out-dir outputs/pred   --print-result\n</code></pre> <p>Output JSON will appear in <code>outputs/pred/</code>.</p>"},{"location":"jetson_mmdet3d_suite/#4-switching-models","title":"4. Switching Models","text":"<p>Use any config + checkpoint:</p> <pre><code>python3 demo/pcd_demo.py &lt;pcd.bin&gt; &lt;config.py&gt; &lt;model.pth&gt; --out-dir outputs/custom --print-result\n</code></pre>"},{"location":"jetson_mmdet3d_suite/#5-notes","title":"5. Notes","text":"<ul> <li>Do not upgrade torch/numpy/mmcv inside this container.</li> <li>This image is meant as a baseline for 3D LiDAR perception on Jetson.</li> <li>You may layer ROS 2, Isaac ROS, or custom pipelines on top.</li> </ul>"},{"location":"vision_suite_guide/","title":"Vision Suite (YOLOv8 + mmdetection3d) \u2014 Docker Usage Guide","text":"<p>This guide shows how to use the combined CUDA Docker image that bundles: - ROS\u00a02 Humble Desktop (rviz2, cv_bridge, image_transport, vision_msgs, etc.) - Ultralytics YOLOv8 for 2D image/video detection - OpenMMLab mmdetection3d (PointPillars example) for 3D LiDAR detection - Your ROS\u00a02 workspace at <code>/ws</code> (can be prebaked or mounted for dev)</p>"},{"location":"vision_suite_guide/#tldr","title":"TL;DR","text":"<pre><code># Build the image (from repo root)\ndocker build -f docker/Dockerfile.vision -t vision-suite:cu118 .\n\n# Run with GPU + X11 for RViz or image windows\nxhost +local:root\ndocker run --rm -it --gpus all   -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix:rw   --device /dev/dri:/dev/dri   --name vision-suite vision-suite:cu118\n</code></pre> <p>Inside the container you can run: - YOLOv8 on an image/video (CLI or Python) - mmdetection3d demo on a KITTI point cloud - ROS\u00a02 YOLO node (<code>image_demo/video_yolo</code>) to publish annotated images to RViz</p>"},{"location":"vision_suite_guide/#image-layout-inside-container","title":"Image Layout (Inside Container)","text":"<pre><code>/home/ros/\n  \u251c\u2500 mmdetection3d/                # OpenMMLab 3D detection repo (precloned)\n  \u2502   \u251c\u2500 checkpoints/              # Pre-downloaded PointPillars weights\n  \u2502   \u2514\u2500 tests/data/kitti/000000.bin  # Sample point cloud\n  \u251c\u2500 Nvidia-Isaac-ROS/             # (Optional) Isaac ROS repo clone\n  \u2514\u2500 outputs/                      # Suggested folder for saving results\n\n/ws/                               # ROS 2 colcon workspace\n  \u251c\u2500 src/ROS2-Tutorial/            # Your repo (prebaked or mounted)\n  \u251c\u2500 build/ install/ log/          # Colcon artifacts after build\n  \u2514\u2500 ...\n/opt/ros/humble/                   # ROS 2 Humble installation\n</code></pre> <p>If <code>/ws/src/ROS2-Tutorial</code> is empty, bind-mount your repo or switch Dockerfile to prebake it (see below).</p>"},{"location":"vision_suite_guide/#run-modes","title":"Run Modes","text":""},{"location":"vision_suite_guide/#1-prebaked-workspace-one-shot-use","title":"1) Prebaked workspace (one-shot use)","text":"<p>If your Dockerfile contains: <pre><code>COPY . /ws/src/ROS2-Tutorial\nRUN bash -lc \"source /opt/ros/${ROS_DISTRO}/setup.bash &amp;&amp; cd /ws &amp;&amp; colcon build --symlink-install\"\n</code></pre> then the image already contains the built ROS packages. Just run the container and launch your nodes.</p>"},{"location":"vision_suite_guide/#2-dev-mode-mount-sources-and-build-inside","title":"2) Dev mode (mount sources and build inside)","text":"<p>Use this for rapid iteration: <pre><code>docker run --rm -it --gpus all   -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix:rw   --device /dev/dri:/dev/dri   -v ~/Desktop/thesi/ROS2-Tutorial:/ws/src/ROS2-Tutorial   --name vision-suite vision-suite:cu118 bash\n\n# inside container\nsource /opt/ros/humble/setup.bash\ncd /ws &amp;&amp; colcon build --symlink-install\nsource /ws/install/setup.bash\n</code></pre></p>"},{"location":"vision_suite_guide/#yolov8-quick-tests-no-ros","title":"YOLOv8 \u2014 Quick Tests (No ROS)","text":""},{"location":"vision_suite_guide/#detect-on-a-single-image","title":"Detect on a single image","text":"<pre><code>cd ~\nwget -O sample.jpg https://ultralytics.com/images/bus.jpg\nyolo detect predict model=yolov8n.pt source=sample.jpg show=True save=True\n# results saved in runs/detect/predict/\n</code></pre>"},{"location":"vision_suite_guide/#detect-on-a-video-file","title":"Detect on a video file","text":"<pre><code>wget -O sample.mp4 https://samplelib.com/lib/preview/mp4/sample-5s.mp4\nyolo detect predict model=yolov8n.pt source=sample.mp4 show=True save=True\n</code></pre>"},{"location":"vision_suite_guide/#minimal-python-snippet","title":"Minimal Python snippet","text":"<pre><code>python3 - &lt;&lt;'PY'\nfrom ultralytics import YOLO\nmodel = YOLO(\"yolov8n.pt\")\nres = model(\"sample.jpg\", show=True)\nprint(res[0].boxes)\nPY\n</code></pre> <p>If a window fails to open, make sure you ran the container with <code>-e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix:rw --device /dev/dri:/dev/dri</code>.</p>"},{"location":"vision_suite_guide/#mmdetection3d-pointpillars-demo","title":"mmdetection3d \u2014 PointPillars Demo","text":"<pre><code># save results in a writable directory\nmkdir -p ~/outputs\n\ncd ~/mmdetection3d\npython3 demo/pcd_demo.py   configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py   checkpoints/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class_20210831_063303-3d3f69ab.pth   tests/data/kitti/000000.bin   --device cuda:0   --out-dir ~/outputs\n</code></pre> <p>If you see a permission error writing under the repo, use <code>~/outputs</code> or run once: <pre><code>sudo chown -R ros:ros ~/mmdetection3d\n</code></pre></p>"},{"location":"vision_suite_guide/#ros-2-yolo-node-rviz","title":"ROS\u00a02 YOLO Node + RViz","text":"<p><code>image_demo/video_yolo.py</code> reads a video/camera, runs YOLO, and publishes: - <code>/image_yolo_raw</code> (original frames) - <code>/image_yolo_annotated</code> (frames with boxes)</p> <pre><code># Build (skip if prebaked)\nsource /opt/ros/humble/setup.bash\ncd /ws &amp;&amp; colcon build --symlink-install\nsource /ws/install/setup.bash\n\n# Get a sample video + weights\ncd ~\nwget -O sample.mp4 https://samplelib.com/lib/preview/mp4/sample-5s.mp4\nwget -O yolov8n.pt https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt\n\n# Run the node\nros2 run image_demo video_yolo --ros-args   -p video_path:=/home/ros/sample.mp4   -p model_path:=/home/ros/yolov8n.pt   -p conf_thres:=0.25\n</code></pre> <p>In another terminal: <pre><code>rviz2\n</code></pre> Add Image display -&gt; set topic to <code>/image_yolo_annotated</code>.</p> <p>If <code>video_yolo</code> isn\u2019t found: ensure <code>setup.py</code> includes <code>video_yolo = image_demo.video_yolo:main</code>, then rebuild + <code>source /ws/install/setup.bash</code>.</p>"},{"location":"vision_suite_guide/#typical-issues-fixes","title":"Typical Issues &amp; Fixes","text":"<p>No GUI / OpenGL errors in RViz Run with: <code>-e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix:rw --device /dev/dri:/dev/dri</code> and <code>xhost +local:root</code> on the host.</p> <p>Package found but \u201cNo executable found\u201d Add the console script in <code>image_demo/setup.py</code>: <pre><code>entry_points={'console_scripts': [\n  'video_yolo = image_demo.video_yolo:main',\n]}\n</code></pre> Rebuild + <code>source /ws/install/setup.bash</code>.</p> <p>Permission denied writing outputs Use a user-owned directory, e.g. <code>~/outputs</code>, or <code>sudo chown -R ros:ros &lt;path&gt;</code>.</p> <p>ultralytics / torch not found These are pre-installed; if you changed Python envs, verify with <pre><code>python3 -c \"import ultralytics, torch; print('ok')\"\n</code></pre></p>"},{"location":"vision_suite_guide/#dockerfile-tweaks-prebake-vs-mount","title":"Dockerfile Tweaks (Prebake vs Mount)","text":"<p>Prebake your ROS workspace (no mounts at runtime): <pre><code>COPY . /ws/src/ROS2-Tutorial\nRUN bash -lc \"source /opt/ros/${ROS_DISTRO}/setup.bash &amp;&amp; cd /ws &amp;&amp; colcon build --symlink-install &amp;&amp; chown -R ${USERNAME}:${USERNAME} /ws\"\n</code></pre></p> <p>Dev mode (recommended while iterating): - Mount your repo with <code>-v ~/Desktop/thesi/ROS2-Tutorial:/ws/src/ROS2-Tutorial</code> - Rebuild with <code>colcon build</code> inside the container</p>"},{"location":"vision_suite_guide/#summary","title":"Summary","text":"<ul> <li>Use <code>vision-suite:cu118</code> for a single container that runs YOLOv8, mmdetection3d, and ROS\u00a02.</li> <li>Choose prebaked (stable) or dev (flexible) workflow.</li> <li>Use RViz to visualize <code>/image_yolo_annotated</code> and <code>mmdetection3d</code> to test 3D inference.</li> <li>Save outputs to <code>~/outputs</code> to avoid permission issues.</li> </ul>"},{"location":"vision_suite_readme/","title":"ROS 2 Vision Suite","text":""},{"location":"vision_suite_readme/#yolo-2d-mmdetection3d-3d-lidar-core-ros-2-demos","title":"YOLO (2D) + MMDetection3D (3D LiDAR) + Core ROS 2 Demos","text":"<p>All running inside a single GPU-accelerated container</p> <p>This document explains how to build and run the three major ROS 2 perception packages included in this workspace:</p> Package Description mmdetect_lidar_demo Real-time 3D LiDAR detection using MMDetection3D (PointPillars, SECOND, CenterPoint, etc.) image_demo YOLOv8 2D detection pipeline over images/video ros2demo Basic ROS 2 publisher/subscriber example <p>All packages are tested inside the container: <code>ambarishgk007/ros2-vision-suite:cu118</code> (ROS 2 Humble + PyTorch 2.1.2 + MMCV 2.1.0 + MMDetection3D 1.4.0 + YOLOv8).</p>"},{"location":"vision_suite_readme/#1-running-the-container","title":"1. Running the Container","text":""},{"location":"vision_suite_readme/#10-docker-pull","title":"1.0 Docker pull","text":"<pre><code>docker pull ambarishgk007/ros2-vision-suite:cu118\n</code></pre>"},{"location":"vision_suite_readme/#11-linux-with-gui-support","title":"1.1 Linux (with GUI support)","text":"<pre><code>xhost +local:root\n\ndocker run --name ros2-mmdetect-test \\\n  --gpus all \\\n  --network host \\\n  --shm-size=8g \\\n  -e DISPLAY=$DISPLAY \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix:rw \\\n  -w /workspace/ROS2-Tutorial \\\n  -it ambarishgk007/ros2-vision-suite:cu118 bash\n</code></pre> <p>Inside the container:</p> <pre><code>source /opt/ros/humble/setup.bash\n</code></pre>"},{"location":"vision_suite_readme/#2-building-the-ros-2-workspace","title":"2. Building the ROS 2 Workspace","text":"<pre><code>cd /workspace/ROS2-Tutorial\n\nsource /opt/ros/humble/setup.bash\ncolcon build --packages-select \\\n  ros2demo \\\n  image_demo \\\n  mmdetect_lidar_demo\n\nsource install/setup.bash\n</code></pre> <p>Verify: <pre><code>ros2 pkg list | grep -E \"ros2demo|image_demo|mmdetect_lidar_demo\"\n</code></pre></p>"},{"location":"vision_suite_readme/#3-lidar-detection-demo-mmdetect_lidar_demo","title":"3. LiDAR Detection Demo (<code>mmdetect_lidar_demo</code>)","text":""},{"location":"vision_suite_readme/#what-the-node-does","title":"What the node does","text":"<ul> <li>Subscribes to <code>/velodyne_points</code></li> <li>Converts to NumPy</li> <li>Runs MMDetection3D inference</li> <li>Publishes:</li> <li><code>/bounding_box_edges</code> \u2192 <code>Marker</code></li> <li><code>/lidar_bounding_boxes</code> \u2192 <code>MarkerArray</code></li> </ul>"},{"location":"vision_suite_readme/#31-start-lidar-rosbag","title":"3.1 Start LiDAR rosbag","text":"<pre><code>docker exec -it ros2-mmdetect-test bash\nsource /opt/ros/humble/setup.bash\ncd ~/Desktop/thesi/ROS2-Tutorial\npip install gdown\n# Start clean\nrm -rf multilidarcalib12\nmkdir multilidarcalib12\ncd multilidarcalib12\n\n# metadata.yaml\nwget \"https://drive.google.com/uc?export=download&amp;id=11MwxBbos8EQRUS6XjXPbnRBSrOqXhUC6\" \\\n     -O metadata.yaml\n\n# bag file: mutlilidarcalib12_0.db3.zstd (\u2248480 MB)\ngdown --fuzzy \"https://drive.google.com/file/d/1qeVQGDzyb72oqKI1ZYWFKiv5jW-6iQdG/view\" \\\n      -O mutlilidarcalib12_0.db3.zstd\n\ncd ..\ncd ~/Desktop/thesi/ROS2-Tutorial\n\n# Start clean\nrm -rf multilidarcalib13\nmkdir multilidarcalib13\ncd multilidarcalib13\n\n# metadata.yaml\nwget \"https://drive.google.com/uc?export=download&amp;id=1GTPLLlX4QLvC1_7FCHnvYmRNH1YUuhHt\" \\\n     -O metadata.yaml\n\n# bag file: NOTE the spelling \"mutli...\" to match metadata.yaml\ngdown --fuzzy \"https://drive.google.com/file/d/1q4szohuHOTi6MAvWRQXlXD-00gCYgvAK/view\" \\\n      -O mutlilidarcalib13_0.db3.zstd\n\ncd ..\n</code></pre>"},{"location":"vision_suite_readme/#repo-structure","title":"Repo Structure","text":"<p><pre><code>ROS2-Tutorial/\n  multilidarcalib12/\n    metadata.yaml\n    mutlilidarcalib12_0.db3.zstd\n\n  multilidarcalib13/\n    metadata.yaml\n    mutlilidarcalib13_0.db3.zstd\n</code></pre> <pre><code>cd ~/Desktop/thesi/ROS2-Tutorial\n\nros2 bag info multilidarcalib12\nros2 bag play multilidarcalib12\n# OR run the other \nros2 bag info multilidarcalib13\nros2 bag play multilidarcalib13\n</code></pre> Check: <pre><code>ros2 topic list | grep velodyne\nros2 topic echo /velodyne_points\n</code></pre></p>"},{"location":"vision_suite_readme/#32-run-lidar-detector","title":"3.2 Run LiDAR detector","text":"<pre><code>source install/setup.bash\nros2 run mmdetect_lidar_demo mmdetect_lidar_demo\n</code></pre>"},{"location":"vision_suite_readme/#33-visualize-in-rviz2","title":"3.3 Visualize in RViz2","text":"<ol> <li>Fixed Frame \u2192 <code>velodyne</code></li> <li>Add:</li> <li>PointCloud2 \u2192 <code>/velodyne_points</code></li> <li>Marker \u2192 <code>/bounding_box_edges</code></li> <li>MarkerArray \u2192 <code>/lidar_bounding_boxes</code></li> </ol>"},{"location":"vision_suite_readme/#4-switching-the-lidar-model","title":"4. Switching the LiDAR Model","text":"<p>Already supported via ROS params:</p> <pre><code>ros2 run mmdetect_lidar_demo mmdetect_lidar_demo \\\n  --ros-args \\\n  -p config_file:=/workspace/mmdetection3d/configs/second/...py \\\n  -p checkpoint_file:=/workspace/mmdetection3d/checkpoints/...pth\n</code></pre> <p>You may also use a params YAML or a launch file.</p> <p>Supports: - PointPillars (default) - SECOND - CenterPoint - Part-A2 - PV-RCNN - Any MMDetection3D model</p>"},{"location":"vision_suite_readme/#5-yolo-image-demo-image_demo","title":"5. YOLO Image Demo (<code>image_demo</code>)","text":""},{"location":"vision_suite_readme/#included-nodes","title":"Included nodes:","text":"Node Purpose <code>video_publisher.py</code> Publishes images from video <code>video_yolo.py</code> Runs YOLOv8 and publishes annotated frames <code>image_relay.py</code> Pass-through image relay <code>image_filler_publisher.py</code> Publishes placeholder image"},{"location":"vision_suite_readme/#51-run-yolo-demo","title":"5.1 Run YOLO demo","text":"<pre><code>ros2 launch image_demo yolo_demo.launch.py\n</code></pre>"},{"location":"vision_suite_readme/#52-rviz2-visualization","title":"5.2 RViz2 visualization","text":"<p><pre><code>rviz2\n</code></pre>  Add: - Image \u2192 <code>/image_yolo</code></p> <p>Or use:</p> <pre><code>rqt_image_view\n</code></pre>"},{"location":"vision_suite_readme/#6-basic-ros-2-demo-ros2demo","title":"6. Basic ROS 2 Demo (<code>ros2demo</code>)","text":"<p>Publisher:</p> <pre><code>ros2 run ros2demo ros2demo_publisher\n</code></pre> <p>Subscriber:</p> <pre><code>ros2 run ros2demo ros2demo_subscriber\n</code></pre> <p>Echo:</p> <pre><code>ros2 topic echo /topic\n</code></pre>"},{"location":"vision_suite_readme/#7-running-all-demos-together","title":"7. Running All Demos Together","text":"<p>Terminal 1 \u2014 YOLO: <pre><code>ros2 launch image_demo yolo_demo.launch.py\n</code></pre></p> <p>Terminal 2 \u2014 LiDAR: <pre><code>ros2 run mmdetect_lidar_demo mmdetect_lidar_demo\n</code></pre></p> <p>Terminal 3 \u2014 rosbag: <pre><code>ros2 bag play your_lidar_bag\n</code></pre></p> <p>RViz: <pre><code>rviz2\n</code></pre></p> <p>Add: - <code>/velodyne_points</code> - <code>/bounding_box_edges</code> - <code>/lidar_bounding_boxes</code> - <code>/image_yolo</code></p>"},{"location":"vision_suite_readme/#8-summary","title":"8. Summary","text":"<p>This container provides: - \u2714 Real-time LiDAR 3D detection (MMDetection3D) - \u2714 Real-time YOLOv8 image detection - \u2714 A complete ROS 2 workspace - \u2714 GPU-accelerated inference - \u2714 Launch files and visualization pipelines - \u2714 Dynamic model switching via ROS parameters</p> <p>Perfect for robotics perception research, autonomy pipelines, or Jetson development.</p>"},{"location":"yolo_example/","title":"YOLO Example (ROS\u00a02 + Docker)","text":"<p>This example shows how to run a pre-baked YOLOv8 demo inside a Docker image, view annotated video in RViz, and understand the node structure so you can tweak it for your projects. No Docker build steps here\u2014just pull and run the published image.</p> <p>Image used: <code>docker pull ambarishgk007/ros2-yolo:humble</code></p>"},{"location":"yolo_example/#what-youll-run","title":"What you\u2019ll run","text":"<ul> <li>A ROS\u00a02 node <code>video_yolo</code> that:</li> <li>Reads a video (<code>.mp4</code>) with OpenCV.</li> <li>Runs YOLOv8 object detection (Ultralytics).</li> <li>Publishes annotated frames to <code>/image_yolo</code> (<code>sensor_msgs/Image</code>).</li> <li>Publishes detections (class + confidence + bbox) to:<ul> <li><code>/detections_yolo</code> (<code>vision_msgs/Detection2DArray</code>) if available or</li> <li><code>/detections_yolo_json</code> (<code>std_msgs/String</code>) as a JSON fallback.</li> </ul> </li> <li>RViz opens with a pre-configured view that subscribes to <code>/image_yolo</code>.</li> </ul>"},{"location":"yolo_example/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux desktop with Docker and X11.</li> <li>Allow X11 access (once per session):   <pre><code>xhost +local:root\n</code></pre></li> </ul>"},{"location":"yolo_example/#quickstart-pull-run","title":"Quickstart (pull &amp; run)","text":"<p>Pull the prebaked image and run the example (no mounts required):</p> <pre><code>docker pull ambarishgk007/ros2-yolo:humble\n\ndocker run --rm -it   --env DISPLAY=$DISPLAY   --env QT_X11_NO_MITSHM=1   -v /tmp/.X11-unix:/tmp/.X11-unix:rw   --name ros2-yolo   ambarishgk007/ros2-yolo:humble\n</code></pre> <p>What happens</p> <ul> <li>Launch file starts: <code>ros2 launch image_demo yolo_demo.launch.py</code></li> <li>The node reads <code>/ws/src/ROS2-Tutorial/examples/data/sample.mp4</code> inside the image.</li> <li>Detections are rendered on frames \u2192 <code>/image_yolo</code>.</li> <li>RViz opens and displays <code>/image_yolo</code> automatically.</li> </ul> <p>If you see OpenGL warnings (e.g., \u201cfailed to load driver: iris\u201d), the container will fall back to software rendering. You can ignore these or add <code>--device /dev/dri:/dev/dri</code> to try hardware GL on Intel/AMD.</p>"},{"location":"yolo_example/#use-your-own-data-mount-override","title":"Use your own data (mount + override)","text":"<p>Mount a folder from your host and pass paths via launch args:</p> <pre><code>docker run --rm -it   --env DISPLAY=$DISPLAY   -v /tmp/.X11-unix:/tmp/.X11-unix:rw   -v $(pwd)/examples/data:/data   --name ros2-yolo   ambarishgk007/ros2-yolo:humble   ros2 launch image_demo yolo_demo.launch.py     video_path:=/data/sample.mp4     model_path:=/data/yolov8n.pt     conf_thres:=0.35\n</code></pre> <ul> <li><code>video_path</code>: any OpenCV-readable video (.mp4, etc.).  </li> <li><code>model_path</code>: YOLOv8 model weights (e.g., <code>yolov8n.pt</code>).  </li> <li><code>conf_thres</code>: detection confidence threshold (0\u20131).</li> </ul>"},{"location":"yolo_example/#topics","title":"Topics","text":"Topic Type Description <code>/image_yolo</code> <code>sensor_msgs/Image</code> Annotated frames with bounding boxes <code>/detections_yolo</code> <code>vision_msgs/Detection2DArray</code> Structured detections (if <code>vision_msgs</code> present) <code>/detections_yolo_json</code> <code>std_msgs/String</code> JSON list of <code>{label, conf, bbox_xyxy}</code> <p>List topics / inspect messages: <pre><code>ros2 topic list\nros2 topic echo /detections_yolo\n</code></pre></p>"},{"location":"yolo_example/#node-architecture-parameters","title":"Node architecture &amp; parameters","text":""},{"location":"yolo_example/#video_yolo-node-python-rclpy","title":"<code>video_yolo</code> node (Python, <code>rclpy</code>)","text":"<p>Responsibilities 1. Capture frames with OpenCV (<code>cv2.VideoCapture</code>). 2. Run YOLOv8 inference (Ultralytics <code>YOLO(model_path)</code>; <code>predict(conf=...)</code>). 3. Draw boxes + labels on a copy of the frame. 4. Publish annotated <code>sensor_msgs/Image</code> via cv_bridge. 5. Publish detections:    - Preferred: <code>vision_msgs/Detection2DArray</code> (class + score + bbox).    - Fallback: JSON list on <code>std_msgs/String</code> if <code>vision_msgs</code> is not installed.</p> <p>Parameters (declared &amp; read via ROS 2 params) - <code>video_path</code> (string): absolute path to the input video. - <code>model_path</code> (string): path to <code>.pt</code> weights. - <code>conf_thres</code> (float): confidence threshold (default 0.25).</p> <p>QoS / Frequency - Publishes at the video\u2019s FPS (falls back to ~30 FPS if metadata is odd). - QoS depth = 10 on image &amp; detection pubs (good default for RViz).</p> <p>The node tries to loop the video when it reaches the end.</p>"},{"location":"yolo_example/#rviz-configuration","title":"RViz configuration","text":"<p>The launch file opens RViz with a minimal config that subscribes to <code>/image_yolo</code>. If you prefer manual control: <pre><code>rviz2\n</code></pre> Then: Add \u2192 Image \u2192 Image Topic = <code>/image_yolo</code>.</p>"},{"location":"yolo_example/#how-the-image-is-structured-for-understandingtweaks","title":"How the image is structured (for understanding/tweaks)","text":"<p>You don\u2019t need this to run the example\u2014but it helps when customizing.</p> <ul> <li>Base: <code>osrf/ros:humble-desktop</code> (Ubuntu 22.04 + RViz + ROS 2 tools).  </li> <li>System ROS packages: <code>cv_bridge</code>, <code>vision_msgs</code>, <code>image_transport</code>.  </li> <li>Python deps live in a virtualenv at <code>/opt/venv</code> (to avoid apt/pip conflicts):  </li> <li><code>opencv-python</code>, <code>ultralytics</code> (with <code>torch</code>, <code>torchvision</code>).  </li> <li>The image sets:     <pre><code>PATH=/opt/venv/bin:$PATH\nPYTHONPATH=/opt/venv/lib/python3.10/site-packages:$PYTHONPATH\n</code></pre>     so ament/ROS entry points can import Ultralytics.</li> <li>Workspace layout inside the container:</li> <li><code>/ws/src/ROS2-Tutorial/</code> \u2192 your repo copy (including <code>image_demo</code>, <code>ros2demo</code>, <code>examples/data</code>).  </li> <li>Built with <code>colcon build --symlink-install</code>.</li> <li>Entrypoint:</li> <li>Sources <code>/opt/ros/humble/setup.bash</code> and <code>/ws/install/setup.bash</code>.  </li> <li>Default <code>CMD</code> (what runs when you <code>docker run</code> the image): <pre><code>ros2 launch image_demo yolo_demo.launch.py     video_path:=/ws/src/ROS2-Tutorial/examples/data/sample.mp4     model_path:=/ws/src/ROS2-Tutorial/examples/data/yolov8n.pt     conf_thres:=0.25\n</code></pre></li> </ul>"},{"location":"yolo_example/#customizing-for-your-needs","title":"Customizing for your needs","text":"<ul> <li>Change topics: edit the publishers in <code>video_yolo.py</code> (e.g., publish to <code>/camera/image</code> to integrate with other stacks).  </li> <li>Different model: mount or bake your own <code>.pt</code> and pass as <code>model_path</code>.  </li> <li>Filter classes: post-filter detections by <code>label</code> or <code>cls_id</code> before drawing/publishing.  </li> <li>Publish raw frames too: add another publisher on <code>/image_raw</code> (no annotations).  </li> <li>Switch to live camera: replace <code>cv2.VideoCapture(video_path)</code> with a device index (<code>0</code>) or GStreamer pipeline.  </li> <li>Structured detections: prefer <code>vision_msgs/Detection2DArray</code> for downstream consumers like tracking/fusion.  </li> <li>RViz layout: copy the RViz config (under <code>image_demo/rviz</code>) and add more panels/views.</li> </ul>"},{"location":"yolo_example/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>RViz shows warnings about OpenGL: Often harmless (software rendering). Try <code>--device /dev/dri:/dev/dri</code> if available.  </li> <li>No video output: check you mounted data and passed correct <code>video_path</code>.  </li> <li>Slow on CPU: try <code>--gpus all</code> (if you have CUDA support on the host) or use a smaller model (e.g., <code>yolov8n.pt</code>).  </li> <li>Can\u2019t see detections: increase <code>conf_thres</code> downwards (e.g., <code>0.2</code>) or check topics with <code>ros2 topic list</code>.</li> </ul>"},{"location":"yolo_example/#next-steps","title":"Next steps","text":"<ul> <li>Add a tracker node (e.g., SORT/DeepSORT) subscribing to <code>/detections_yolo</code>.  </li> <li>Publish segmentation masks with a segmentation model.  </li> <li>Replace video source with a ROS camera driver (e.g., ZED/USB cam) and run detections live.</li> </ul> <p>Happy hacking! \ud83d\ude80</p>"}]}